{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting painting authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will build a model to predict authors of paintings. I will use the [dataset](https://www.kaggle.com/datasets/ikarus777/best-artworks-of-all-time) from Kaggle that has 8,000+ paintings by 50 most famous artists to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with exploring the information about the paintings in the dataset. Luckily, there is a csv file summarizing information about all paintings in the dataset and their authors. I will read it as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = pd.read_csv(r'data\\artists.csv').drop(columns=['id', 'bio', 'wikipedia'])\n",
    "\n",
    "#sorting the dataframe by the number of paintings in the dataset\n",
    "artists.sort_values('paintings', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vincent van Gogh has the most paintings in the dataset, followed by Edgar Degas and Pablo Picasso. The difference between artists withe the most and the least paintings is large, meaning that the dataset is imbalanced. I'll deal with it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each artist in the dataset is associated with one or more genres. Let's have a look at the distributrion of artists' genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all genres from the genre column\n",
    "genre_count = artists.genre.str.get_dummies(sep=',').sum().sort_values()\n",
    "\n",
    "#plotting the number of genres in the dataset\n",
    "genre_count.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressionism and Post-Impressionism are the most popular genres, if we only count individual artists. There are also plenty of artists associated with different periods and types of Renaissance art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paintings is stored on my local machine. Below, I display some of the paintings, selected randomly. The code used here is adapted from this Stackoverflow [answer](https://stackoverflow.com/a/60443998)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the list of all filepaths\n",
    "train_folder = 'data\\images'\n",
    "images = {}\n",
    "for folder in os.listdir(train_folder):\n",
    "    for image in os.listdir(train_folder + '/' + folder):\n",
    "        filename = os.path.join(train_folder, folder, image)\n",
    "        author = folder.replace('_', ' ')\n",
    "        images[filename] = author\n",
    "        \n",
    "plt.figure(1, figsize=(12, 8))\n",
    "\n",
    "#displaying nine randomly selected images\n",
    "n = 0\n",
    "for i in range(9):\n",
    "    n += 1\n",
    "    random_img = random.choice(list(images.keys()))\n",
    "    imgs = plt.imread(random_img)\n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(imgs)\n",
    "    plt.title(images[random_img])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will build a simple deep learning network using TensorFlow to try to predict authors of the paintings. I will start with generating a dataset from the images in the folder on my local machine. I will resize all images to the 180x180 size and split them into 80 percent training and 20 percent validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (180, 180)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    'data\\artists',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=17,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    "    )\n",
    "\n",
    "valid_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    'data\\artists',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=17,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the relatvely small number of images in the dataset, I will use data augmentation to mitigate overfitting and allow the model generalize better to unseen images. I will use three data augmentation layers that adds random noise to images and randomly increased or decreases brightness and contrast of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomBrightness(0.5),\n",
    "    tf.keras.layers.RandomContrast(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(original, augmented):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(original[0].numpy().astype(\"int32\"))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Augmented image')\n",
    "    plt.imshow(augmented[0].numpy().astype(\"int32\"))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I will compare the original resized images with the same image that was augmented using GaussianNoise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_ds))\n",
    "augmented_image = tf.keras.layers.RandomBrightness(0.5)(\n",
    "            tf.expand_dims(image[0], 0), training=True\n",
    "            )\n",
    "visualize(image, augmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_ds))\n",
    "augmented_image = tf.keras.layers.RandomContrast(0.5)(\n",
    "            tf.expand_dims(image[0], 0), training=True\n",
    "            )\n",
    "visualize(image, augmented_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the data aumentation layer as the first layer of the network. I will add several convolutional layers, as well as max pooling and dropouts layers for regularization. The final layer will output a classification prediction with probabilities of a painting being authored by all of 50 possible artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(50, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling the model, I will train the network for 15 epochs and then evaluate the accuracy of its predictions on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=valid_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/67256122\n",
    "predictions = np.array([])\n",
    "labels =  np.array([])\n",
    "\n",
    "for x, y in valid_ds:\n",
    "    predictions = np.concatenate([predictions, np.argmax(model.predict(x, verbose=0), axis=-1)])\n",
    "    labels = np.concatenate([labels, y.numpy()])\n",
    "\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m(labels, predictions).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model's prediction for the validation set is rather low. It peaked at around 33 percent after 11 epochs. This is not surpising for the rather simple neural network archirecture given the complicated nature of the task, high number of possible classification labels, and rather limited number of training examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the accuracy of predictions on the train and test sets changed after each epochs, I'll create a simple plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history, epochs):\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, epochs + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history, epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model's prediction for the validation set is rather low. It peaked at around 32-33 percent after 10 epochs and flattened out after that. This is not surpising for the rather simple neural network archirecture given the complicated nature of the task, high number of possible classification labels, and rather limited number of training examples. Meanwhil, the training accuracy started below the validation accuracy but increased at a much faster rate, leading to significant overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #getting predicted probabilities\n",
    "# y_pred_proba = model.predict(valid_ds)\n",
    "\n",
    "# #getting predicted classes - https://github.com/keras-team/keras/issues/5961\n",
    "# y_pred = y_pred_proba.argmax(axis=-1)\n",
    "\n",
    "# #getting actual classes - https://stackoverflow.com/a/62823218\n",
    "# y = np.concatenate([y for _, y in valid_ds], axis=0)\n",
    "\n",
    "artists_alphabetic = artists.sort_values('name').reset_index(drop=True)\n",
    "artists_dict = dict(zip(artists_alphabetic.index, artists_alphabetic.name))\n",
    "\n",
    "pred_artists = [artists_dict[k] for k in predictions]\n",
    "actual_artists = [artists_dict[k] for k in labels]\n",
    "results_df = pd.DataFrame({'predicted': pred_artists, 'actual': actual_artists})\n",
    "results_df['result'] = results_df.predicted == results_df.actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(results_df.predicted == results_df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_by_artist = results_df.groupby('actual')['result'].mean()\n",
    "correct_by_artist = pd.DataFrame({'name': correct_by_artist.index, 'share_correct': correct_by_artist.values})\n",
    "\n",
    "artist_paintings_dict = dict(zip(artists.name, artists.paintings))\n",
    "\n",
    "correct_by_artist['total_paintings'] = correct_by_artist.name.map(artist_paintings_dict)\n",
    "correct_by_artist.sort_values('share_correct', ascending=False)\n",
    "\n",
    "sns.regplot(y=correct_by_artist['share_correct'], x=correct_by_artist['total_paintings'], ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_by_artist.sort_values('share_correct', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the model, I will use transfer learning. I will use a pre-trained ResNet-50 convolutional neural network as a base model. To adapt it to the painting dataset and the classification task, I will add the 50-label classification layer on the top of the network, along with a average pooling and a dropout layer, and train it on the dataset. As the ResNet-50 network accepts 224x244 images, I will re-create a training and validation dataset with the necessary image dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'data\\artists',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=17,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'data\\artists',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=17,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_rn = tf.keras.applications.resnet50.ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "for layers in base_model_rn.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "dropout_layer = tf.keras.layers.Dropout(0.2)\n",
    "prediction_layer = tf.keras.layers.Dense(22, activation='softmax')\n",
    "\n",
    "model_rn = tf.keras.Sequential([\n",
    "    base_model_rn,\n",
    "    global_average_layer,\n",
    "    dropout_layer,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model_rn.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "204/204 [==============================] - 418s 2s/step - loss: 2.4286 - accuracy: 0.2794 - val_loss: 1.9936 - val_accuracy: 0.3760\n",
      "Epoch 2/10\n",
      "204/204 [==============================] - 413s 2s/step - loss: 1.8323 - accuracy: 0.4155 - val_loss: 1.8821 - val_accuracy: 0.4000\n",
      "Epoch 3/10\n",
      "204/204 [==============================] - 416s 2s/step - loss: 1.6249 - accuracy: 0.4649 - val_loss: 1.9651 - val_accuracy: 0.3815\n",
      "Epoch 4/10\n",
      "204/204 [==============================] - 422s 2s/step - loss: 1.4817 - accuracy: 0.5000 - val_loss: 1.9775 - val_accuracy: 0.3914\n",
      "Epoch 5/10\n",
      "204/204 [==============================] - 414s 2s/step - loss: 1.3722 - accuracy: 0.5438 - val_loss: 1.9400 - val_accuracy: 0.3957\n",
      "Epoch 6/10\n",
      "204/204 [==============================] - 410s 2s/step - loss: 1.2988 - accuracy: 0.5557 - val_loss: 1.9628 - val_accuracy: 0.3994\n",
      "Epoch 7/10\n",
      "204/204 [==============================] - 410s 2s/step - loss: 1.2512 - accuracy: 0.5769 - val_loss: 1.9041 - val_accuracy: 0.4338\n",
      "Epoch 8/10\n",
      "204/204 [==============================] - 414s 2s/step - loss: 1.1898 - accuracy: 0.5915 - val_loss: 1.9718 - val_accuracy: 0.3994\n",
      "Epoch 9/10\n",
      "153/204 [=====================>........] - ETA: 1:22 - loss: 1.1410 - accuracy: 0.6158"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m model_rn\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      4\u001b[0m   styles_train_ds,\n\u001b[0;32m      5\u001b[0m   validation_data\u001b[39m=\u001b[39;49mstyles_valid_ds,\n\u001b[0;32m      6\u001b[0m   epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model_rn.fit(\n",
    "  styles_train_ds,\n",
    "  validation_data=styles_valid_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8125 files belonging to 22 classes.\n",
      "Using 6500 files for training.\n",
      "Found 8125 files belonging to 22 classes.\n",
      "Using 1625 files for validation.\n"
     ]
    }
   ],
   "source": [
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "styles_train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'data\\styles',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=17,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "styles_valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'data\\styles',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=17,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_en = tf.keras.applications.vgg16.VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "for layers in base_model_en.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "model_en = tf.keras.Sequential([\n",
    "    base_model_en,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(22, activation='softmax')\n",
    "])\n",
    "\n",
    "model_en.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder in os.listdir('data/styles/'):\n",
    "#     files = sorted(os.listdir(f'data/styles/{folder}/'))\n",
    "#     sample_number = len(files) // 10\n",
    "#     random_files = random.sample(files, sample_number)\n",
    "#     excessive_files = list(set(files) - set(random_files))\n",
    "#     for file in excessive_files:\n",
    "#         os.remove(f'data/styles/{folder}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "204/204 [==============================] - 775s 4s/step - loss: 5.5573 - accuracy: 0.1797 - val_loss: 4.2121 - val_accuracy: 0.2382\n",
      "Epoch 2/10\n",
      "204/204 [==============================] - 777s 4s/step - loss: 3.3249 - accuracy: 0.2840 - val_loss: 3.3227 - val_accuracy: 0.2757\n",
      "Epoch 3/10\n",
      "204/204 [==============================] - 759s 4s/step - loss: 2.5390 - accuracy: 0.3538 - val_loss: 3.0525 - val_accuracy: 0.3009\n",
      "Epoch 4/10\n",
      "204/204 [==============================] - 729s 4s/step - loss: 2.1676 - accuracy: 0.4051 - val_loss: 2.9161 - val_accuracy: 0.3120\n",
      "Epoch 5/10\n",
      "204/204 [==============================] - 728s 4s/step - loss: 1.8947 - accuracy: 0.4466 - val_loss: 2.7527 - val_accuracy: 0.3317\n",
      "Epoch 6/10\n",
      "204/204 [==============================] - 730s 4s/step - loss: 1.7254 - accuracy: 0.4749 - val_loss: 2.6396 - val_accuracy: 0.3538\n",
      "Epoch 7/10\n",
      "204/204 [==============================] - 732s 4s/step - loss: 1.6041 - accuracy: 0.5002 - val_loss: 2.6964 - val_accuracy: 0.3458\n",
      "Epoch 8/10\n",
      "167/204 [=======================>......] - ETA: 1:49 - loss: 1.5215 - accuracy: 0.5253"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m----> 3\u001b[0m history_en \u001b[39m=\u001b[39m model_en\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      4\u001b[0m   styles_train_ds,\n\u001b[0;32m      5\u001b[0m   validation_data\u001b[39m=\u001b[39;49mstyles_valid_ds,\n\u001b[0;32m      6\u001b[0m   epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\oleks\\paintings\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "history_en = model_en.fit(\n",
    "  styles_train_ds,\n",
    "  validation_data=styles_valid_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en.save('efficientnetb0.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the necessary layers on the top of the pre-trained network, I will train these additional layers on the training dataset for 20 epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will plot how the training and validation accuracies changed with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history, epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of predictions on the validation set increased significantly with the use of the pre-trained network. The validation accuracy peaked at"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will train the model on the full dataset. Given that the validation accuracy ceased to improve significatly after ... epochs, I will train the model for this number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size= (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "full_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'data\\images',\n",
    "    seed=17,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "history = model.fit(\n",
    "  full_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, I will save it. I will need the saved model to develop a simple web app that will predict authors of famous paintings. I will use HuggingFace spaces for this purpose. The app will be available [here](https://huggingface.co/spaces/osydorchuk/painting_authors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('efficientnetb0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = tf.keras.utils.load_img(r'test_data\\vangoghmuseum-s0005V1962-800.jpg')\n",
    "# input_arr = tf.keras.utils.img_to_array(image)\n",
    "# input = tf.image.resize(input_arr, [224, 224])\n",
    "# input = tf.keras.applications.resnet50.preprocess_input(input)\n",
    "# input = np.array([input])\n",
    "# predictions = model.predict(input)\n",
    "# predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d34c7ccc81fa4295a96834bb3412f7fea67b2cb8c706ad9e914ebcf2701cd46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
